registerDoParallel(cl)
rm(list = ls())
setwd("C:/Users/Litan Li/Desktop/electronidex/brand_preference")
###############
# Load packages
################
library(caret)
library(corrplot)
library(readr)
######################
# Parallel processing
######################
library(doParallel)
# Check number of cores and workers available
detectCores()
cl <- makeCluster(detectCores()-1, type='PSOCK')
# Start parallel cluster
registerDoParallel(cl)
CompleteResponses <- read.csv("CompleteResponses.csv", stringsAsFactors = FALSE, header=T)
# prediction/new data
SurveyIncomplete <- read.csv("SurveyIncomplete.csv", stringsAsFactors = FALSE, header=T)
CompleteResponses$elevel  <- as.ordered(CompleteResponses$elevel)
CompleteResponses$car     <- as.factor(CompleteResponses$car)
CompleteResponses$zipcode <- as.factor(CompleteResponses$zipcode)
CompleteResponses$brand   <- as.factor(CompleteResponses$brand)
str(CompleteResponses)
str(SurveyIncomplete)
SurveyIncomplete$elevel  <- as.ordered(SurveyIncomplete$elevel)
SurveyIncomplete$car     <- as.factor(SurveyIncomplete$car)
SurveyIncomplete$zipcode <- as.factor(SurveyIncomplete$zipcode)
str(SurveyIncomplete)
# remove label column from prediction/new data - we'll predict this
SurveyIncomplete$brand <- NULL
inTraining <- createDataPartition(CompleteResponses$brand, p=0.8, list=FALSE)
trainSet <- CompleteResponses[inTraining,]
testSet <- CompleteResponses[-inTraining,]
# scale features
scaleParamsTrain <- preProcess(trainSet[, c(1,2,6)],
method = c("center", "scale"))
print(scaleParamsTrain)
trainSetS <- predict(scaleParamsTrain, trainSet)
testSetS <- predict(scaleParamsTrain, testSet) # scaled with training set means and std. devs.
predictionSetS <- predict(scaleParamsTrain, SurveyIncomplete) # scaled with train set means and std. devs.
str(trainSetS)
warnings()
str(testSetS)
str(predictionSetS)
rm(list = c('CompleteResponses', 'SurveyIncomplete', 'inTraining', 'trainSet',
'testSet', 'scaleParamsTrain'))
rangerFit <- readRDS("rangerFit.rds")
print(rangerFit)
confusionMatrix(rangerFit, norm = "none") # confusion matrix for the hold-out samples
ggplot(rangerFit)
ggplot(rangerFit, metric = "Kappa")
ggplot(rangerFit)
varImp(rangerFit) # most important features
print(rangerFit$finalModel)
trainPred <- predict(rangerFit, trainSetS[ ,1:6])
confusionMatrix(data = trainPred, reference = trainSetS$brand)
testPred <- predict(rangerFit, testSetS[ ,1:6])
confusionMatrix(data = testPred, reference = testSetS$brand)
predictionPred <- predict(rangerFit, predictionSetS)
SurveyIncomplete$brand <- predictionPred
SurveyIncomplete <- read.csv("SurveyIncomplete.csv", stringsAsFactors = FALSE, header=T)
SurveyIncomplete <- read.csv("survey_incomplete.csv", stringsAsFactors = FALSE, header=T)
complete_responses <- read.csv("complete_responses.csv", stringsAsFactors = FALSE, header=T)
# prediction/new data
survey_incomplete <- read.csv("survey_incomplete.csv", stringsAsFactors = FALSE, header=T)
str(complete_responses)  # 10,000 obs. of  7 variables
summary(complete_responses)
hist(complete_responses$salary)
hist(complete_responses$age)
hist(complete_responses$elevel)
hist(complete_responses$car)
hist(complete_responses$zipcode)
hist(complete_responses$credit)
hist(complete_responses$brand)
corrAll = cor(complete_responses, use = "all.obs", method = "pearson")
corrplot(corrAll, order = "hclust") # sorts based on level of collinearity
corrplot(corrAll, method = "circle")
qqnorm(complete_responses$salary)
qqplot(complete_responses$salary, complete_responses$age, plot.it = TRUE)
anyNA(complete_responses)
is.na(complete_responses)
str(survey_incomplete)  # 5,000 obs. of  7 variables
summary(survey_incomplete)
hist(survey_incomplete$salary)
hist(survey_incomplete$age)
hist(survey_incomplete$elevel)
hist(survey_incomplete$car)
hist(survey_incomplete$zipcode)
hist(survey_incomplete$credit)
qqnorm(survey_incomplete$credit) #normal quantile plot.
qqplot(survey_incomplete$salary, survey_incomplete$age, plot.it = TRUE) # q-q plot.
is.na(survey_incomplete)
str(complete_responses)
complete_responses$elevel  <- as.ordered(complete_responses$elevel)
complete_responses$car     <- as.factor(complete_responses$car)
complete_responses$zipcode <- as.factor(complete_responses$zipcode)
complete_responses$brand   <- as.factor(complete_responses$brand)
str(complete_responses)
str(survey_incomplete)
survey_incomplete$elevel  <- as.ordered(survey_incomplete$elevel)
survey_incomplete$car     <- as.factor(survey_incomplete$car)
survey_incomplete$zipcode <- as.factor(survey_incomplete$zipcode)
str(survey_incomplete)
survey_incomplete$brand <- NULL
inTraining <- createDataPartition(complete_responses$brand, p=0.8, list=FALSE)
trainSet <- complete_responses[inTraining,]
testSet <- complete_responses[-inTraining,]
scaleParamsTrain <- preProcess(trainSet[, c(1,2,6)],
method = c("center", "scale"))
print(scaleParamsTrain)
trainSetS <- predict(scaleParamsTrain, trainSet)
testSetS <- predict(scaleParamsTrain, testSet) # scaled with training set means and std. devs.
predictionSetS <- predict(scaleParamsTrain, survey_incomplete) # scaled with train set means and std. devs.
rm(list = ls())
setwd("C:/Users/Litan Li/Desktop/electronidex/brand_preference")
library(caret)
library(corrplot)
library(readr)
######################
# Parallel processing
######################
library(doParallel)
# Check number of cores and workers available
detectCores()
cl <- makeCluster(detectCores()-1, type='PSOCK')
# Start parallel cluster
registerDoParallel(cl)
complete_responses <- read.csv("complete_responses.csv", stringsAsFactors = FALSE, header=T)
# prediction/new data
survey_incomplete <- read.csv("survey_incomplete.csv", stringsAsFactors = FALSE, header=T)
str(complete_responses)  # 10,000 obs. of  7 variables
summary(complete_responses)
hist(complete_responses$salary)
hist(complete_responses$age)
hist(complete_responses$elevel)
hist(complete_responses$car)
hist(complete_responses$zipcode)
hist(complete_responses$credit)
hist(complete_responses$brand)
# check for collinearity - correlation matrix
corrAll = cor(complete_responses, use = "all.obs", method = "pearson")
corrplot(corrAll, order = "hclust") # sorts based on level of collinearity
corrplot(corrAll, method = "circle")
# Does not seem to be any linearly related variables.
# normal quantile plot. If linear, then attribute values are normally distributed.
qqnorm(complete_responses$salary)
# q-q plot. If linear line on y=x, then similar distribution. If linear but not
# on y=x, then distributions are linearly related.
qqplot(complete_responses$salary, complete_responses$age, plot.it = TRUE)
# check for missing values
anyNA(complete_responses)
is.na(complete_responses)
#--- prediction/new data ---#
str(survey_incomplete)  # 5,000 obs. of  7 variables
summary(survey_incomplete)
# plot
hist(survey_incomplete$salary)
hist(survey_incomplete$age)
hist(survey_incomplete$elevel)
hist(survey_incomplete$car)
hist(survey_incomplete$zipcode)
hist(survey_incomplete$credit)
qqnorm(survey_incomplete$credit) #normal quantile plot.
qqplot(survey_incomplete$salary, survey_incomplete$age, plot.it = TRUE) # q-q plot.
# check for missing values
anyNA(survey_incomplete)
is.na(survey_incomplete)
str(complete_responses)
complete_responses$elevel  <- as.ordered(complete_responses$elevel)
complete_responses$car     <- as.factor(complete_responses$car)
complete_responses$zipcode <- as.factor(complete_responses$zipcode)
complete_responses$brand   <- as.factor(complete_responses$brand)
str(complete_responses)
str(survey_incomplete)
survey_incomplete$elevel  <- as.ordered(survey_incomplete$elevel)
survey_incomplete$car     <- as.factor(survey_incomplete$car)
survey_incomplete$zipcode <- as.factor(survey_incomplete$zipcode)
str(survey_incomplete)
# remove label column from prediction/new data - we'll predict this
survey_incomplete$brand <- NULL
# Split into training/val set and test set. We will use k-fold cross validation
# when training, so training and validation examples will both be pulled from
# trainSet
in_training <- createDataPartition(complete_responses$brand, p=0.8, list=FALSE)
train_set <- complete_responses[inTraining,]
test_set <- complete_responses[-inTraining,]
# feature scaling
scale_params_train <- preProcess(train_set[, c(1,2,6)],
method = c("center", "scale"))
print(scale_params_train)
train_set <- predict(scale_params_train, train_set)
test_set <- predict(scale_params_train, test_set) # scaled with training set means and std. devs.
prediction_set <- predict(scale_params_train, survey_incomplete) # scaled with train set means and std. devs.
str(complete_responses)
complete_responses$elevel  <- as.ordered(complete_responses$elevel)
complete_responses$car     <- as.factor(complete_responses$car)
complete_responses$zipcode <- as.factor(complete_responses$zipcode)
complete_responses$brand   <- as.factor(complete_responses$brand)
str(complete_responses)
str(survey_incomplete)
survey_incomplete$elevel  <- as.ordered(survey_incomplete$elevel)
survey_incomplete$car     <- as.factor(survey_incomplete$car)
survey_incomplete$zipcode <- as.factor(survey_incomplete$zipcode)
str(survey_incomplete)
# remove label column from prediction/new data - we'll predict this
survey_incomplete$brand <- NULL
in_training <- createDataPartition(complete_responses$brand, p=0.8, list=FALSE)
train_set <- complete_responses[in_training,]
test_set <- complete_responses[-in_training,]
scale_params_train <- preProcess(train_set[, c(1,2,6)],
method = c("center", "scale"))
print(scale_params_train)
train_set <- predict(scale_params_train, train_set)
test_set <- predict(scale_params_train, test_set) # scaled with training set means and std. devs.
prediction_set <- predict(scale_params_train, survey_incomplete) # scaled with train set means and std. devs.
write.csv(train_set, "train_set.csv", row.names = FALSE)
write.csv(test_set, "test_set.csv", row.names = FALSE)
write.csv(predictionSetS, "prediction_set.csv", row.names = FALSE)
write.csv(prediction_set, "prediction_set.csv", row.names = FALSE)
ranger_fit_control <- trainControl(method = 'cv', number = 10,
summaryFunction = defaultSummary)
modelLookup('ranger')
ranger_grid <- expand.grid(mtry = c(2,4,6,8,10),
splitrule = c('gini'),
min.node.size = c(7,9))
nrow(ranger_grid)
tic <- Sys.time()
ranger_fit <- train(x = train_set[ ,1:6], y = train_set$brand,
method = "ranger",
trControl = ranger_fitControl,
tuneGrid = ranger_grid)
toc <- Sys.time()
runtime <- toc - tic
tic <- Sys.time()
ranger_fit <- train(x = train_set[ ,1:6], y = train_set$brand,
method = "ranger",
trControl = ranger_fit_control,
tuneGrid = ranger_grid)
ranger_fit <- readRDS("ranger_fit.rds")
print(ranger_fit)
confusionMatrix(ranger_fit, norm = "none") # confusion matrix for the hold-out samples
ggplot(ranger_fit)
ggplot(ranger_fit, metric = "Kappa")
print(ranger_fit$finalModel)
train_Pred <- predict(ranger_fit, train_set[ ,1:6])
confusionMatrix(data = train_Pred, reference = train_set$brand)
train_set[ ,1:6]
confusionMatrix(data = train_Pred, reference = train_set$brand)
confusionMatrix(data = train_Pred, reference = train_set$brand)
print(ranger_fit)
confusionMatrix(ranger_fit, norm = "none") # confusion matrix for the hold-out samples
print(ranger_fit$finalModel)
train_Pred <- predict(ranger_fit, train_set[ ,1:6])
confusionMatrix(data = train_Pred, reference = train_set$brand)
test_Pred <- predict(ranger_fit, test_set[ ,1:6])
confusionMatrix(data = test_Pred, reference = test_set$brand)
prediction_pred <- predict(ranger_fit, prediction_set)
survey_incomplete$brand <- prediction_pred
write.csv(survey_incomplete, "predicted.csv")
summary(prediction_pred)
rm(list = ls())
setwd("C:/Users/Litan Li/Desktop/electronidex/product_lineup")
library(caret)
library(corrplot)
library(readr)
######################
# Parallel processing
######################
library(doParallel)
# Check number of cores and workers available
detectCores()
cl <- makeCluster(detectCores()-1, type='PSOCK')
# start parallel cluster
registerDoParallel(cl)
existing_products <- read.csv("existingproductattributes2017.csv",
stringsAsFactors = FALSE, header=T)
# prediction/new data
new_products <- read.csv("newproductattributes2017.csv",
stringsAsFactors = FALSE, header=T)
barplot(table(existing_products$ProductType))
hist(existing_products$Price)
hist(existing_products$x5StarReviews)
hist(existing_products$x4StarReviews)
hist(existing_products$x3StarReviews)
summary(existing_products)
str(existing_products)  # 80 obs. of  18 variables
plot(existing_products$Price, existing_products$Volume)
plot(existing_products$x5StarReviews, existing_products$Volume) # linear
plot(existing_products$x4StarReviews, existing_products$Volume)
plot(existing_products$x3StarReviews, existing_products$Volume)
plot(existing_products$x2StarReviews, existing_products$Volume)
plot(existing_products$x1StarReviews, existing_products$Volume)
plot(existing_products$PositiveServiceReview, existing_products$Volume)
plot(existing_products$NegativeServiceReview, existing_products$Volume)
plot(existing_products$Recommendproduct, existing_products$Volume)
plot(existing_products$BestSellersRank, existing_products$Volume)
plot(existing_products$ShippingWeight, existing_products$Volume)
plot(existing_products$ProductDepth, existing_products$Volume)
corrAll = cor(existing_products[ ,c(3:11, 13:18)], use = "all.obs", method = "pearson")
corrplot(corrAll, order = "hclust") # sorts based on level of collinearity
corrplot(corrAll, method = "circle")
print(corrAll)
existing_products[ ,c(3:11, 13:18)]
View(existing_products)
summary(existing_products)
str(existing_products)  # 80 obs. of  18 variables
str(existing_products)
print(corrAll)
qqnorm(CompleteResponses$salary)
print(corrAll)
str(SurveyIncomplete)  # 5,000 obs. of  7 variables
str(new_products)  # 5,000 obs. of  7 variables
hist(existing_products$x1StarReviews)
plot(existing_products$x5StarReviews, existing_products$Volume)
plot(existing_products$x4StarReviews, existing_products$Volume)
plot(existing_products$x3StarReviews, existing_products$Volume)
plot(existing_products$Recommendproduct, existing_products$Volume)
str(new_products)  # 24 obs. of 18 variables
barplot(table(new_products$ProductType))
hist(new_products$Price)
hist(new_products$x5StarReviews)
hist(new_products$x4StarReviews)
hist(new_products$x3StarReviews)
hist(new_products$x2StarReviews)
hist(new_products$x1StarReviews)
hist(new_products$PositiveServiceReview)
hist(new_products$NegativeServiceReview)
hist(new_products$Recommendproduct)
hist(new_products$BestSellersRank)
hist(new_products$ShippingWeight)
hist(new_products$ProductDepth)
hist(new_products$ProductWidth)
hist(new_products$ProductHeight)
hist(new_products$ProfitMargin)
hist(new_products$Volume)
plot(new_products$Price, new_products$Volume)
plot(new_products$x4StarReviews, new_products$Volume)
View(new_products)
is.na(new_products)
anyNA(new_products)
anyNA(existing_products[ ,12])
existing_products[ ,12]
anyNA(existing_products)
is.na(existing_products)
anyNA(new_products)
existing_products$BestSellersRank <- NULL
existing_products$BestSellersRank <- NULL
existing_products$ProductNum <- NULL
existing_products$BestSellersRank <- NULL
existing_products$ProductNum <- NULL
corrAll = cor(existing_products, use = "all.obs", method = "pearson")
write.csv(corrAll, "correlation_matrix_existing_productsD.csv")
corrplot(corrAll, order = "hclust") # sorts based on level of collinearity
corrplot(corrAll, method = "circle")
corrAll = cor(existing_products, use = "all.obs", method = "pearson")
write.csv(corrAll, "existing_products_corr_matrix.csv")
corrplot(corrAll, order = "hclust") # sorts based on level of collinearity
corrplot(corrAll, method = "circle")
existing_products$x4StarReviews <- NULL
existing_products$x2StarReviews <- NULL
existing_products$x1StarReviews <- NULL
corr13v = cor(existing_products, use = "all.obs", method = "pearson")
str(existing_products)
corr13v = cor(existing_products[, c(1:13)], use = "all.obs", method = "pearson")
corrplot(corrAll, order = "hclust") # sorts based on level of collinearity
corrplot(corrAll, method = "circle")
corrplot(corr13v, order = "hclust") # sorts based on level of collinearity
corr13v = cor(existing_products[, c(1:13)], use = "all.obs", method = "pearson")
existing_products[, c(1:13)]
str(existing_products)
corr13v = cor(existing_products[, c(2:13)], use = "all.obs", method = "pearson")
corrplot(corr13v, order = "hclust") # sorts based on level of collinearity
write.csv(corr13v, "corr_matrix_13v.csv")
str(existing_products)
existing_products$ProductType <- as.ordered(existing_products$ProductType)
str(existing_products)
existing_products$ProductType <- as.factor(existing_products$ProductType)
str(existing_products)
existing_products$ProductType <- as.character(existing_products$ProductType)
str(existing_products)
existing_products$ProductType <- as.factor(existing_products$ProductType)
str(existing_products)
dummies <- dummyVars(~., existing_products, fullRank = TRUE)
View(dummies)
existing_productsD <- data.frame(predict(dummies, existing_products))
View(existing_productsD)
existing_products$ProductType.unique()
unique(existing_products$ProductType)
View(existing_productsD)
existing_products <- data.frame(predict(dummies, existing_products))
str(existing_products)
in_training <- createDataPartition(existing_products$Volume, p=0.8, list=FALSE)
train_set <- complete_responses[in_training,]
test_set <- complete_responses[-in_training,]
train_set <- existing_products[in_training,]
test_set <- existing_products[-in_training,]
View(train_set)
train_set[, 13:22]
train_set[, 12:22]
train_set[, 11:22]
train_set[, 12:22]
View(train_set)
scale_params_train <- preProcess(train_set[, 12:22],
method = c("center", "scale"))
print(scale_params_train)
train_set <- predict(scale_params_train, train_set)
new_products$ProductNum <- NULL
new_products$x4StarReviews <- NULL
new_products$x2StarReviews <- NULL
new_products$x1StarReviews <- NULL
new_products$ProductType <- as.factor(new_products$ProductType)
dummies2 <- dummyVars(~., new_products, fullRank = TRUE)
new_products <- data.frame(predict(dummies2, new_products))
new_products <- predict(scale_params_train, new_products)
new_products$BestSellersRank <- NULL
prediction_set <- new_products
View(prediction_set)
write.csv(train_set, "train_set.csv", row.names = FALSE)
write.csv(test_set, "test_set.csv", row.names = FALSE)
write.csv(prediction_set, "prediction_set.csv", row.names = FALSE)
summary(prediction_set)
str(train_set)
train_set      <- read.csv("train_set.csv")
str(train_set)
modelLookup('lm')
View(train_set)
tic <- Sys.time()
lm_fit <- train(x = train_set[ ,1:22], y = train_set$Volume,
method = 'lm',
trControl = lm_fit_control)
toc <- Sys.time()
runtime <- toc - tic
lm_fit_control <- trainControl(method = 'cv', number = 10,
summaryFunction = defaultSummary)
tic <- Sys.time()
lm_fit <- train(x = train_set[ ,1:22], y = train_set$Volume,
method = 'lm',
trControl = lm_fit_control)
toc <- Sys.time()
runtime <- toc - tic
print(lm_fit)
print(lm_fit)
confusionMatrix(lm_fit, norm = "none")
print(lm_fit)
ggplot(lm_fit)
varImp(lm_fit) # most important features
print(lm_fit$finalModel)
train_pred <- predict(lm_fit, train_set[ ,1:22])
train_pred
train_pred - train_set$Volume
train_pred
train_set$Volume
class(train_pred - train_set$Volume)
(train_pred - train_set$Volume)^2
train_pred - train_set$Volume
sum(train_pred - train_set$Volume)^2
(sum(train_pred - train_set$Volume)^2/65)^0.5
test_pred <- predict(lm_fit, train_set[ ,1:22])
(sum(test_pred - test_set$Volume)^2/23)^0.5
test_pred
test_set$Volume
test_pred - test_set$Volume
test_set[ ,1:22]
test_pred <- predict(lm_fit, test_set[ ,1:22])
(sum(test_pred - test_set$Volume)^2/23)^0.5
test_pred
test_set$Volume
test_set[ ,1:22]
test_pred <- predict(lm_fit, test_set[ ,1:22])
test_pred
test_set$Volume
train_pred
View(train_set)
View(test_set)
test_set[ ,1:22]
test_set[ ,1:22]
View(train_set)
View(test_set)
test_set <- predict(scale_params_train, test_set)
test_pred <- predict(lm_fit, test_set[ ,1:22])
(sum(test_pred - test_set$Volume)^2/23)^0.5
saveRDS(lm_fit, "lm_fit.rds")
train_set_residuals = train_set$Volume - train_pred
train_set_residuals
plot(train_pred, train_set_residuals)
print(lm_fit$finalModel)
summary(lm_fit)
hist(train_set_residuals)
qqnorm(train_set_residuals)
test_set_residuals = test_set$Volume - test_pred
plot(test_pred, test_set_residuals) # relatively homoscedastic
hist(test_set_residuals)
qqnorm(test_set_residuals) # residuals are relatively normally distributed
survey_incomplete$brand <- prediction_pred
write.csv(survey_incomplete, "predicted.csv")
new_products$Volume <- prediction_pred
prediction_pred <- predict(lm_fit, prediction_set)
new_products$Volume <- prediction_pred
View(new_products)
newproductattributes2017 <- read.csv("newproductattributes2017.csv",
stringsAsFactors = FALSE, header=T)
write.csv(newproductattributes2017, "predicted_new_product_volumes.csv")
summary(prediction_pred)
stopCluster(cl)
prediction_pred
View(new_products)
newproductattributes2017$Volume <- prediction_pred
newproductattributes2017$Volume <- prediction_pred
write.csv(newproductattributes2017, "predicted_new_product_volumes.csv")
